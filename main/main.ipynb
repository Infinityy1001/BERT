{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify text with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s understand with code how to build BERT with PyTorch. \n",
    "\n",
    "We will break the entire program into 4 sections:\n",
    "\n",
    "1. Preprocessing\n",
    "2. Building model\n",
    "3. Loss and Optimization\n",
    "4. Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05779837816953659,\n",
       "  'token': 3337,\n",
       "  'token_str': 'boys',\n",
       "  'sequence': 'the boys went to the store'},\n",
       " {'score': 0.04151124134659767,\n",
       "  'token': 3057,\n",
       "  'token_str': 'girls',\n",
       "  'sequence': 'the girls went to the store'},\n",
       " {'score': 0.03639920428395271,\n",
       "  'token': 2500,\n",
       "  'token_str': 'others',\n",
       "  'sequence': 'the others went to the store'},\n",
       " {'score': 0.03291618078947067,\n",
       "  'token': 2273,\n",
       "  'token_str': 'men',\n",
       "  'sequence': 'the men went to the store'},\n",
       " {'score': 0.03129786252975464,\n",
       "  'token': 2048,\n",
       "  'token_str': 'two',\n",
       "  'sequence': 'the two went to the store'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"The [MASK] went to the store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633d7c9c6a4747b18d6e1b5e982a9859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743ffbb99f324b9c8eb1d4537d7262a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5258e768ddab4aacbc92f9b972858974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3acb1d66fd4de1ada5f596b4838ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465a19587b584e21b6d6c3f0dd24c4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46501a28e4a463f93992ac65fe0ad99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e321a4326c476cb4a64a39e3a49595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "\n",
      "The capital of France is Paris. This is the largest part of France; it's also the busiest in Europe. By the way, the capital was originally named after France's founder, Napoleon Bonaparte\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger un pipeline de génération de texte avec GPT-2\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Demander une question\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "# Générer une réponse\n",
    "response = generator(question, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Afficher la réponse générée\n",
    "print(response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of Egypt?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Générer une réponse\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m generator(question, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Afficher la réponse générée\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of Egypt?\"\n",
    "\n",
    "# Générer une réponse\n",
    "response = generator(question, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Afficher la réponse générée\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Victor Hugo?\n",
      "\n",
      "(From: The Amazing World of Harry Potter Fan Fic Archive)\n",
      "\n",
      "Virgil: The Man Who Dived Down [The Man Who Saved the World]\n",
      "\n",
      "(From: Harry Potter Book\n"
     ]
    }
   ],
   "source": [
    "question1= 'Who is Victor Hugo'\n",
    "\n",
    "reponse1 = generator(question1, max_length = 50, num_return_sequences = 1)\n",
    "\n",
    "print(reponse1[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "       'Hello, how are you? I am Romeo.n'\n",
    "       'Hello, Romeo My name is Juliet. Nice to meet you.n'\n",
    "       'Nice meet you too. How are you today?n'\n",
    "       'Great. My baseball team won the competition.n'\n",
    "       'Oh Congratulations, Julietn'\n",
    "       'Thanks you Romeo'\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will clean the data by:\n",
    "\n",
    "- Making the sentences into lower case.\n",
    "- Creating vocabulary. Vocabulary is a list of unique words in the document. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.sub(\"[.,!?-]\", '', text.lower()).split('n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "   word_dict[w] = i + 4\n",
    "   number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "   vocab_size = len(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is taken care of, we need to create a function that formats the input sequences for three types of embeddings: token embedding, segment embedding, and position embedding.\n",
    "\n",
    "What is token embedding?\n",
    "\n",
    "For instance, if the sentence is “The cat is walking. The dog is barking”, then the function should create a sequence in the following manner: “[CLS] the cat is walking [SEP] the dog is barking”. \n",
    "\n",
    "After that, we convert everything to an index from the word dictionary. So the previous sentence would look something like “[1, 5, 7, 9, 10, 2, 5, 6, 9, 11]”. Keep in mind that 1 and 2 are [CLS] and [SEP] respectively. \n",
    "\n",
    "What is segment embedding?\n",
    "\n",
    "A segment embedding separates two sentences from each other and they are generally defined as 0 and 1. \n",
    "\n",
    "What is position embedding?\n",
    "\n",
    "A position embedding gives position to each embedding in a sequence. \n",
    "\n",
    "We will create a function for position embedding later. \n",
    "\n",
    "![embeddings](assets/embeddings.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step will be to create masking. \n",
    "\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. But keep in mind that you don’t assign masks to the special tokens. For that, we will use conditional statements.\n",
    "\n",
    "Once we replace 15% of the words with [MASK] tokens, we will add padding. Padding is usually done to make sure that all the sentences are of equal length. For instance, if we take the sentence :\n",
    "\n",
    " “The cat is walking. The dog is barking at the tree”\n",
    "\n",
    "then with padding, it will look like this: \n",
    "\n",
    "“[CLS] The cat is walking [PAD] [PAD] [PAD]. [CLS] The dog is barking at the tree.” \n",
    "\n",
    "The length of the first sentence is equal to the length of the second sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def make_batch():\n",
    "   batch = []\n",
    "   positive = negative = 0\n",
    "   while positive != batch_size/2 or negative != batch_size/2:\n",
    "       tokens_a_index, tokens_b_index= random.randrange(len(sentences)), random.randrange(len(sentences))\n",
    "\n",
    "       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "       input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "       segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "       # MASK LM\n",
    "       n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "       cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "       shuffle(cand_maked_pos)\n",
    "       masked_tokens, masked_pos = [], []\n",
    "       for pos in cand_maked_pos[:n_pred]:\n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           if random() < 0.8:  # 80%\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "           elif random() < 0.5:  # 10%\n",
    "               index = random.randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "       # Zero Paddings\n",
    "       n_pad = maxlen - len(input_ids)\n",
    "       input_ids.extend([0] * n_pad)\n",
    "       segment_ids.extend([0] * n_pad)\n",
    "\n",
    "       # Zero Padding (100% - 15%) tokens\n",
    "       if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)\n",
    "\n",
    "       if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "           positive += 1\n",
    "       elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "           negative += 1\n",
    "   return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with next-word prediction, we have to create a label that predicts whether the sentence has a consecutive sentence or not, i.e. IsNext or NotNext. So we assign True for every sentence that precedes the next sentence and we use a conditional statement to do that. \n",
    "\n",
    "For instance, two sentences in a document usually follow each other if they are in context. So assuming the first sentence is A then the next sentence should be A+1. Intuitively we write the code such that if the first sentence positions i.e. tokens_a_index + 1 == tokens_b_index,  i.e. second sentence in the same context, then we can set the label for this input as True. \n",
    "\n",
    "If the above condition is not met i.e. if tokens_a_index + 1 != tokens_b_index then we set the label for this input as False. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a complex model and if it is perceived slowly you lose track of the logic. So it’ll only make sense to explain its component by component and their function.\n",
    "\n",
    "BERT has the following components:\n",
    "\n",
    "1. Embedding layers\n",
    "2. Attention Mask\n",
    "3. Encoder layer\n",
    "\n",
    "        - Multi-head attention\n",
    "\n",
    "        - Scaled dot product attention\n",
    "\n",
    "        - Position-wise feed-forward network\n",
    "        \n",
    "4. BERT (assembling all the components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding layer\n",
    "\n",
    "The embedding is the first layer in BERT that takes the input and creates a lookup table. The parameters of the embedding layers are learnable, which means when the learning process is over the embeddings will cluster similar words together. \n",
    "\n",
    "The embedding layer also preserves different relationships between words such as: semantic, syntactic, linear, and since BERT is bidirectional it will also preserve contextual relationships as well. \n",
    "\n",
    "In the case of BERT, it creates three embeddings for \n",
    "\n",
    "    Token, \n",
    "    Segments and\n",
    "    Position. \n",
    "\n",
    "If you recall we haven’t created a function that takes the input and formats it for position embedding but the formatting for token and segments are completed. So we will take the input and create a position for each word in the sequence. And it looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expand_as(): argument 'other' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m30\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mexpand_as(input_ids))\n",
      "\u001b[1;31mTypeError\u001b[0m: expand_as(): argument 'other' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.arange(30, dtype=torch.long).expand_as(input_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
